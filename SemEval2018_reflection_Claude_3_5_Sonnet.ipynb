{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "KcvE4KCqs1Hb",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "KcvE4KCqs1Hb",
    "outputId": "d38f9a35-da02-4c0b-ffd2-5565dfb82db7",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in /opt/homebrew/Cellar/jupyterlab/4.2.3/libexec/lib/python3.12/site-packages (2.2.3)\n",
      "Requirement already satisfied: openai in /opt/homebrew/Cellar/jupyterlab/4.2.3/libexec/lib/python3.12/site-packages (1.52.1)\n",
      "Requirement already satisfied: together in /opt/homebrew/Cellar/jupyterlab/4.2.3/libexec/lib/python3.12/site-packages (1.3.3)\n",
      "Requirement already satisfied: anthropic in /opt/homebrew/Cellar/jupyterlab/4.2.3/libexec/lib/python3.12/site-packages (0.37.1)\n",
      "Requirement already satisfied: numpy>=1.26.0 in /opt/homebrew/Cellar/jupyterlab/4.2.3/libexec/lib/python3.12/site-packages (from pandas) (2.1.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/homebrew/Cellar/jupyterlab/4.2.3/libexec/lib/python3.12/site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/homebrew/Cellar/jupyterlab/4.2.3/libexec/lib/python3.12/site-packages (from pandas) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /opt/homebrew/Cellar/jupyterlab/4.2.3/libexec/lib/python3.12/site-packages (from pandas) (2024.2)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in /opt/homebrew/Cellar/jupyterlab/4.2.3/libexec/lib/python3.12/site-packages (from openai) (4.4.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /opt/homebrew/Cellar/jupyterlab/4.2.3/libexec/lib/python3.12/site-packages (from openai) (1.9.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /opt/homebrew/Cellar/jupyterlab/4.2.3/libexec/lib/python3.12/site-packages (from openai) (0.27.0)\n",
      "Requirement already satisfied: jiter<1,>=0.4.0 in /opt/homebrew/Cellar/jupyterlab/4.2.3/libexec/lib/python3.12/site-packages (from openai) (0.6.1)\n",
      "Requirement already satisfied: pydantic<3,>=1.9.0 in /opt/homebrew/Cellar/jupyterlab/4.2.3/libexec/lib/python3.12/site-packages (from openai) (2.9.2)\n",
      "Requirement already satisfied: sniffio in /opt/homebrew/Cellar/jupyterlab/4.2.3/libexec/lib/python3.12/site-packages (from openai) (1.3.1)\n",
      "Requirement already satisfied: tqdm>4 in /opt/homebrew/Cellar/jupyterlab/4.2.3/libexec/lib/python3.12/site-packages (from openai) (4.66.5)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.11 in /opt/homebrew/Cellar/jupyterlab/4.2.3/libexec/lib/python3.12/site-packages (from openai) (4.12.2)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.9.3 in /opt/homebrew/Cellar/jupyterlab/4.2.3/libexec/lib/python3.12/site-packages (from together) (3.10.10)\n",
      "Requirement already satisfied: click<9.0.0,>=8.1.7 in /opt/homebrew/Cellar/jupyterlab/4.2.3/libexec/lib/python3.12/site-packages (from together) (8.1.7)\n",
      "Requirement already satisfied: eval-type-backport<0.3.0,>=0.1.3 in /opt/homebrew/Cellar/jupyterlab/4.2.3/libexec/lib/python3.12/site-packages (from together) (0.2.0)\n",
      "Requirement already satisfied: filelock<4.0.0,>=3.13.1 in /opt/homebrew/Cellar/jupyterlab/4.2.3/libexec/lib/python3.12/site-packages (from together) (3.16.1)\n",
      "Requirement already satisfied: pillow<11.0.0,>=10.3.0 in /opt/homebrew/Cellar/jupyterlab/4.2.3/libexec/lib/python3.12/site-packages (from together) (10.4.0)\n",
      "Requirement already satisfied: pyarrow>=10.0.1 in /opt/homebrew/Cellar/jupyterlab/4.2.3/libexec/lib/python3.12/site-packages (from together) (17.0.0)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.31.0 in /opt/homebrew/Cellar/jupyterlab/4.2.3/libexec/lib/python3.12/site-packages (from together) (2.32.3)\n",
      "Requirement already satisfied: rich<14.0.0,>=13.8.1 in /opt/homebrew/Cellar/jupyterlab/4.2.3/libexec/lib/python3.12/site-packages (from together) (13.9.3)\n",
      "Requirement already satisfied: tabulate<0.10.0,>=0.9.0 in /opt/homebrew/Cellar/jupyterlab/4.2.3/libexec/lib/python3.12/site-packages (from together) (0.9.0)\n",
      "Requirement already satisfied: typer<0.13,>=0.9 in /opt/homebrew/Cellar/jupyterlab/4.2.3/libexec/lib/python3.12/site-packages (from together) (0.12.5)\n",
      "Requirement already satisfied: tokenizers>=0.13.0 in /opt/homebrew/Cellar/jupyterlab/4.2.3/libexec/lib/python3.12/site-packages (from anthropic) (0.20.1)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /opt/homebrew/Cellar/jupyterlab/4.2.3/libexec/lib/python3.12/site-packages (from aiohttp<4.0.0,>=3.9.3->together) (2.4.3)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /opt/homebrew/Cellar/jupyterlab/4.2.3/libexec/lib/python3.12/site-packages (from aiohttp<4.0.0,>=3.9.3->together) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/homebrew/Cellar/jupyterlab/4.2.3/libexec/lib/python3.12/site-packages (from aiohttp<4.0.0,>=3.9.3->together) (23.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /opt/homebrew/Cellar/jupyterlab/4.2.3/libexec/lib/python3.12/site-packages (from aiohttp<4.0.0,>=3.9.3->together) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/homebrew/Cellar/jupyterlab/4.2.3/libexec/lib/python3.12/site-packages (from aiohttp<4.0.0,>=3.9.3->together) (6.1.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.12.0 in /opt/homebrew/Cellar/jupyterlab/4.2.3/libexec/lib/python3.12/site-packages (from aiohttp<4.0.0,>=3.9.3->together) (1.16.0)\n",
      "Requirement already satisfied: idna>=2.8 in /opt/homebrew/Cellar/jupyterlab/4.2.3/libexec/lib/python3.12/site-packages (from anyio<5,>=3.5.0->openai) (3.7)\n",
      "Requirement already satisfied: certifi in /opt/homebrew/opt/certifi/lib/python3.12/site-packages (from httpx<1,>=0.23.0->openai) (2024.7.4)\n",
      "Requirement already satisfied: httpcore==1.* in /opt/homebrew/Cellar/jupyterlab/4.2.3/libexec/lib/python3.12/site-packages (from httpx<1,>=0.23.0->openai) (1.0.5)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /opt/homebrew/Cellar/jupyterlab/4.2.3/libexec/lib/python3.12/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.14.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /opt/homebrew/Cellar/jupyterlab/4.2.3/libexec/lib/python3.12/site-packages (from pydantic<3,>=1.9.0->openai) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.23.4 in /opt/homebrew/Cellar/jupyterlab/4.2.3/libexec/lib/python3.12/site-packages (from pydantic<3,>=1.9.0->openai) (2.23.4)\n",
      "Requirement already satisfied: six>=1.5 in /opt/homebrew/Cellar/jupyterlab/4.2.3/libexec/lib/python3.12/site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/homebrew/Cellar/jupyterlab/4.2.3/libexec/lib/python3.12/site-packages (from requests<3.0.0,>=2.31.0->together) (3.3.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/homebrew/Cellar/jupyterlab/4.2.3/libexec/lib/python3.12/site-packages (from requests<3.0.0,>=2.31.0->together) (2.2.2)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /opt/homebrew/Cellar/jupyterlab/4.2.3/libexec/lib/python3.12/site-packages (from rich<14.0.0,>=13.8.1->together) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /opt/homebrew/Cellar/jupyterlab/4.2.3/libexec/lib/python3.12/site-packages (from rich<14.0.0,>=13.8.1->together) (2.18.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /opt/homebrew/Cellar/jupyterlab/4.2.3/libexec/lib/python3.12/site-packages (from tokenizers>=0.13.0->anthropic) (0.26.1)\n",
      "Requirement already satisfied: shellingham>=1.3.0 in /opt/homebrew/Cellar/jupyterlab/4.2.3/libexec/lib/python3.12/site-packages (from typer<0.13,>=0.9->together) (1.5.4)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /opt/homebrew/Cellar/jupyterlab/4.2.3/libexec/lib/python3.12/site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers>=0.13.0->anthropic) (2024.10.0)\n",
      "Requirement already satisfied: packaging>=20.9 in /opt/homebrew/Cellar/jupyterlab/4.2.3/libexec/lib/python3.12/site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers>=0.13.0->anthropic) (24.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/homebrew/Cellar/jupyterlab/4.2.3/libexec/lib/python3.12/site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers>=0.13.0->anthropic) (6.0.1)\n",
      "Requirement already satisfied: mdurl~=0.1 in /opt/homebrew/Cellar/jupyterlab/4.2.3/libexec/lib/python3.12/site-packages (from markdown-it-py>=2.2.0->rich<14.0.0,>=13.8.1->together) (0.1.2)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /opt/homebrew/Cellar/jupyterlab/4.2.3/libexec/lib/python3.12/site-packages (from yarl<2.0,>=1.12.0->aiohttp<4.0.0,>=3.9.3->together) (0.2.0)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.3.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49m/opt/homebrew/Cellar/jupyterlab/4.2.3/libexec/bin/python -m pip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install pandas openai together anthropic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "e1045e13-3cba-441f-bf34-18b11d3646e0",
   "metadata": {
    "id": "e1045e13-3cba-441f-bf34-18b11d3646e0"
   },
   "outputs": [],
   "source": [
    "import openai\n",
    "import re\n",
    "import random\n",
    "import anthropic\n",
    "from collections import Counter\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "62d219a1-6872-4ff1-be2c-eabeba850b08",
   "metadata": {
    "id": "62d219a1-6872-4ff1-be2c-eabeba850b08"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "\n",
    "client = anthropic.Anthropic()\n",
    "import os\n",
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "136e5a33-6f84-4039-9738-c4b7d9e33f4d",
   "metadata": {
    "id": "136e5a33-6f84-4039-9738-c4b7d9e33f4d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    1\n",
      "1    1\n",
      "2    1\n",
      "3    1\n",
      "4    1\n",
      "Name: sarcastic, dtype: int64\n",
      "0    The only thing I got from college is a caffein...\n",
      "1    I love it when professors draw a big question ...\n",
      "2    Remember the hundred emails from companies whe...\n",
      "3    Today my pop-pop told me I was not “forced” to...\n",
      "4    @VolphanCarol @littlewhitty @mysticalmanatee I...\n",
      "Name: tweet, dtype: object\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import glob\n",
    "import os\n",
    "\n",
    "path = \"../data/iSarcasmEval.csv\"\n",
    "df = pd.read_csv(path);\n",
    "\n",
    "answers = df['sarcastic']\n",
    "statements = df['tweet']\n",
    "\n",
    "print (answers.head());\n",
    "print (statements.head());\n",
    "print();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "99fe05e7",
   "metadata": {
    "id": "99fe05e7"
   },
   "outputs": [],
   "source": [
    "outputs =[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "b9d1e110-87dc-493b-8aa3-0bccbc1a6058",
   "metadata": {
    "id": "b9d1e110-87dc-493b-8aa3-0bccbc1a6058"
   },
   "outputs": [],
   "source": [
    "from anthropic import Anthropic\n",
    "\n",
    "def getResultNoPrompt(input, answer):\n",
    "    anthropic = Anthropic()\n",
    "\n",
    "    prompt = f\"\\\"{input}\\\"\\n Determine whether this statement is sarcastic with YES or NO.\"\n",
    "\n",
    "    response = anthropic.messages.create(\n",
    "        model=\"claude-3-5-sonnet-20240620\",\n",
    "        messages=[\n",
    "            {\"role\": \"user\", \"content\": prompt}\n",
    "        ],\n",
    "        max_tokens=3000,\n",
    "        temperature=1.0\n",
    "    )\n",
    "\n",
    "    output = response.content[0].text.strip()\n",
    "    isSarcastic = True if \"yes\" in output.lower() else False\n",
    "    return isSarcastic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "41560bb2-bba6-42ce-9312-92c02dc167e3",
   "metadata": {
    "id": "41560bb2-bba6-42ce-9312-92c02dc167e3"
   },
   "outputs": [],
   "source": [
    "from anthropic import Anthropic\n",
    "\n",
    "def getResultWPrompt(sample_input):\n",
    "   anthropic = Anthropic()\n",
    "   judgeInput = \"\"\n",
    "   output = \"\"\n",
    "\n",
    "   # System prompt for sarcasm justification\n",
    "   sarc_system_prompt = (\"You will be given a message from a political debate on the internet, and will analyze the statement. Repeat back the statement to analyze.\"\n",
    "                        \"Then, analyze the following:\\n\"\n",
    "                        \"-What does the speaker imply about the situation with their statement?\\n\"\n",
    "                        \"-What does the speaker think about the situation?\\n\"\n",
    "                        \"-Are what the speaker implies and what the speaker thinks saying the same thing?\\n\"\n",
    "                        \"Finally, decide if the speaker is pretending to have a certain attitude toward the conversation.\"\n",
    "                       )\n",
    "\n",
    "   sarc_response = anthropic.messages.create(\n",
    "       model=\"claude-3-5-sonnet-20240620\",\n",
    "       system=sarc_system_prompt,\n",
    "       messages=[\n",
    "           {\"role\": \"user\", \"content\": sample_input}\n",
    "       ],\n",
    "       max_tokens=2000,\n",
    "       temperature=1.0\n",
    "   )\n",
    "\n",
    "   judgeInput += \"\\n- - - - - - - - - - - - - - - - - - - - - - - - - - - \\n\"\n",
    "   judgeInput += sarc_response.content[0].text.strip()\n",
    "\n",
    "   reflect_system_prompt = (\"You will be given a message from a political debate on the internet and a preliminary analysis on the statement. Summarize the preliminary analysis\"\n",
    "                          \"Decide whether statement is sarcastic or not by first analyzing the following:\\n\"\n",
    "                          \"\\nThe Implicature - What is implied in the conversation beyond the literal meaning?\"\n",
    "                          \"\\nThe Presuppositions - What information in the conversation is taken for granted?\"\n",
    "                          \"\\nThe intent of the speaker - What do the speaker(s) hope to achieve with their statement and who are the speakers?\\n\"\n",
    "                          \"\\nThe polarity - Does the last sentence have a positive or negative tone?\"\n",
    "                          \"\\nPretense - Is there pretense in the speaker's attitude?\"\n",
    "                          \"\\nMeaning- What is the difference between the literal and implied meaning of the statement?\"\n",
    "                          \"Reflect on the preliminary analysis and what should change, then decide if the statment is sarcastic.\")\n",
    "\n",
    "   reflection_response = anthropic.messages.create(\n",
    "       model=\"claude-3-5-sonnet-20240620\",\n",
    "       system=reflect_system_prompt,\n",
    "       messages=[\n",
    "           {\"role\": \"user\", \"content\": sample_input + \" \" + judgeInput}\n",
    "       ],\n",
    "       max_tokens=4000,\n",
    "       temperature=1.0\n",
    "   )\n",
    "\n",
    "   judgeInput += \"\\n- - - - - - - - - - - - - - - - - - - - - - - - - - - \\n\"\n",
    "   judgeInput += reflection_response.content[0].text.strip()\n",
    "   judgeInput += \"\\n- - - - - - - - - - - - - - - - - - - - - - - - - - - \\n\"\n",
    "   output += (f\"\\nANSWER: {judgeInput}\\n\")\n",
    "\n",
    "   # Clean output prompt to summarize the decision as YES/NO\n",
    "   clean_system_prompt = (\n",
    "       \"You will be given the output of an LLM which decided if a sentence is sarcastic or not. \"\n",
    "       \"Read the output, then summarize the LLM's stance with ONLY a YES (they think the sentence is sarcastic) or NO (they think the sentence is not sarcastic).\"\n",
    "   )\n",
    "\n",
    "   clean_output = anthropic.messages.create(\n",
    "       model=\"claude-3-5-sonnet-20240620\",\n",
    "       system=clean_system_prompt,\n",
    "       messages=[\n",
    "           {\"role\": \"user\", \"content\": judgeInput}\n",
    "       ],\n",
    "       max_tokens=10,\n",
    "       temperature=1.0\n",
    "   )\n",
    "\n",
    "   clean_output = clean_output.content[0].text.strip()\n",
    "   output += (f\"Judgement: {clean_output}\\n\")\n",
    "\n",
    "   # Store output and determine if the statement is sarcastic\n",
    "   outputs.append(output)  # Make sure 'outputs' is defined in your scope\n",
    "   isSarcastic = True if \"yes\" in clean_output.lower() else False\n",
    "   return isSarcastic"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44773b51",
   "metadata": {
    "id": "44773b51"
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "3aed0c29-e06f-4676-8851-80e4a540825b",
   "metadata": {
    "id": "3aed0c29-e06f-4676-8851-80e4a540825b"
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import time\n",
    "\n",
    "def beginPrompting(inputs, answers, with_prompt):#, indices):\n",
    "    print(\"beginning to prompt\")\n",
    "    num_samples = len(indices)\n",
    "\n",
    "    with open(\"SarcasmCorpus_mistakes_V1.txt\" ,\"a\") as file:\n",
    "            file.write (f\"\\nIncorrect Answers: (Full output listed below) \\n- - - - - - - - - - - - - - - - - - - - - - - - - - - \\n\")\n",
    "\n",
    "    # indices = random.sample(range(len(inputs)), num_samples)\n",
    "    # sampled_inputs = [inputs[i] for i in indices]\n",
    "    # sampled_answers = [answers[i] for i in indices]\n",
    "\n",
    "    isSarCt = 0\n",
    "    noSarCt = 0\n",
    "    totNoSarCt = 0\n",
    "    totSarCt = 0\n",
    "\n",
    "    # Changed all of the sampled_inputs to inputs and sampled_answers to answers\n",
    "    \n",
    "    for x in range(num_samples):\n",
    "        time.sleep(5)\n",
    "        isSarcastic = getResultWPrompt(inputs[x]) if with_prompt else getResultNoPrompt(inputs[x])\n",
    "        if answers[x] == 1:\n",
    "            totSarCt += 1\n",
    "            if(isSarcastic):\n",
    "                isSarCt += 1\n",
    "            else:\n",
    "                with open(\"SarcasmCorpus_mistakes_V1.txt\" ,\"a\") as file:\n",
    "                    file.write (f\"\\n{outputs[x]} \\n- - - - - - - - - - - - - - - - - - - - - - - - - - - \\n\")\n",
    "                    file.write (f\"\\nANSWER: {answers[x]}\\n\")\n",
    "        else:\n",
    "            totNoSarCt += 1\n",
    "            if(not isSarcastic):\n",
    "                noSarCt += 1\n",
    "            else:\n",
    "                with open(\"SarcasmCorpus_mistakes_V1.txt\" ,\"a\") as file:\n",
    "                    file.write (f\"\\n{outputs[x]} \\n- - - - - - - - - - - - - - - - - - - - - - - - - - - \\n\")\n",
    "                    file.write (f\"\\nANSWER: {sampled_answers[x]}\\n\")\n",
    "\n",
    "        if with_prompt:\n",
    "            outputs[x]+=(f\"\\nANSWER: {sampled_answers[x]}\\n\")\n",
    "            outputs[x]+=(f\"Prompt sarcastic statement recognition ({isSarCt} total)\\n\")\n",
    "            outputs[x]+=(f\"Total Prompted sarcastic statement  ({totSarCt} total)  \\n\")\n",
    "            outputs[x]+=(f\"Non sarcastic statement recognition ({noSarCt} total)\\n\")\n",
    "            outputs[x]+=(f\"Total Prompted non sarcastic statement ({totNoSarCt} total)\\n\")\n",
    "            # print(\"----------\")\n",
    "            # print(f\"Prompt sarcastic statement recognition ({isSarCt} total)\")\n",
    "            # print(f\"Total Prompted sarcastic statement  ({totSarCt} total)  \")\n",
    "            # print(f\"Non sarcastic statement recognition ({noSarCt} total)\")\n",
    "            # print(f\"Total Prompted non sarcastic statement ({totNoSarCt} total)\")\n",
    "            # print(\"----------\")\n",
    "        print(x);\n",
    "\n",
    "    sar_percentage = (isSarCt / totSarCt * 100) if totSarCt > 0 else 0\n",
    "    no_sar_percentage = (noSarCt / totNoSarCt * 100) if totNoSarCt > 0 else 0\n",
    "\n",
    "    return sar_percentage, no_sar_percentage, totSarCt, totNoSarCt, noSarCt, isSarCt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "75b5f647-b82c-4aa0-a56f-7b651a7847c8",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "75b5f647-b82c-4aa0-a56f-7b651a7847c8",
    "outputId": "9db404e1-67b7-48cc-9419-5cfa4f296303",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "beginning to prompt\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "79\n",
      "80\n",
      "81\n",
      "82\n",
      "83\n",
      "84\n",
      "85\n",
      "86\n",
      "87\n",
      "88\n",
      "89\n",
      "90\n",
      "91\n",
      "92\n",
      "93\n",
      "94\n",
      "95\n",
      "96\n",
      "97\n",
      "98\n",
      "99\n",
      "100\n",
      "101\n",
      "102\n",
      "103\n",
      "104\n",
      "105\n",
      "106\n",
      "107\n",
      "108\n",
      "109\n",
      "110\n",
      "111\n",
      "112\n",
      "113\n",
      "114\n",
      "115\n",
      "116\n",
      "117\n",
      "118\n",
      "119\n",
      "120\n",
      "121\n",
      "122\n",
      "123\n",
      "124\n",
      "125\n",
      "126\n",
      "127\n",
      "128\n",
      "129\n",
      "130\n",
      "131\n",
      "132\n",
      "133\n",
      "134\n",
      "135\n",
      "136\n",
      "137\n",
      "138\n",
      "139\n",
      "140\n",
      "141\n",
      "142\n",
      "143\n",
      "144\n",
      "145\n",
      "146\n",
      "147\n",
      "148\n",
      "149\n",
      "150\n",
      "151\n",
      "152\n",
      "153\n",
      "154\n",
      "155\n",
      "156\n",
      "157\n",
      "158\n",
      "159\n",
      "160\n",
      "161\n",
      "162\n",
      "163\n",
      "164\n",
      "165\n",
      "166\n",
      "167\n",
      "168\n",
      "169\n",
      "170\n",
      "171\n",
      "172\n",
      "173\n",
      "174\n",
      "175\n",
      "176\n",
      "177\n",
      "178\n",
      "179\n",
      "180\n",
      "181\n",
      "182\n",
      "183\n",
      "184\n",
      "185\n",
      "186\n",
      "187\n",
      "188\n",
      "189\n",
      "190\n",
      "191\n",
      "192\n",
      "193\n",
      "194\n",
      "195\n",
      "196\n",
      "197\n",
      "198\n",
      "199\n",
      "200\n",
      "201\n",
      "202\n",
      "203\n",
      "204\n",
      "205\n",
      "206\n",
      "207\n",
      "208\n",
      "209\n",
      "210\n",
      "211\n",
      "212\n",
      "213\n",
      "214\n",
      "215\n",
      "216\n",
      "217\n",
      "218\n",
      "219\n",
      "220\n",
      "221\n",
      "222\n",
      "223\n",
      "224\n",
      "225\n",
      "226\n",
      "227\n",
      "228\n",
      "229\n",
      "230\n",
      "231\n",
      "232\n",
      "233\n",
      "234\n",
      "235\n",
      "236\n",
      "237\n",
      "238\n",
      "239\n",
      "240\n",
      "241\n",
      "242\n",
      "243\n",
      "244\n",
      "245\n",
      "246\n",
      "247\n",
      "248\n",
      "249\n",
      "250\n",
      "251\n",
      "252\n",
      "253\n",
      "254\n",
      "255\n",
      "256\n",
      "257\n",
      "258\n",
      "259\n",
      "260\n",
      "261\n",
      "262\n",
      "263\n",
      "264\n",
      "265\n",
      "266\n",
      "267\n",
      "268\n",
      "269\n",
      "270\n",
      "271\n",
      "272\n",
      "273\n",
      "274\n",
      "275\n",
      "276\n",
      "277\n",
      "278\n",
      "279\n",
      "280\n",
      "281\n",
      "282\n",
      "283\n",
      "284\n",
      "285\n",
      "286\n",
      "287\n",
      "288\n",
      "289\n",
      "290\n",
      "291\n",
      "292\n",
      "293\n",
      "294\n",
      "295\n",
      "296\n",
      "297\n",
      "298\n",
      "299\n",
      "300\n",
      "301\n",
      "302\n",
      "303\n",
      "304\n",
      "305\n",
      "306\n",
      "307\n",
      "308\n",
      "309\n",
      "310\n",
      "311\n",
      "312\n",
      "313\n",
      "314\n",
      "315\n",
      "316\n",
      "317\n",
      "318\n",
      "319\n",
      "320\n",
      "321\n",
      "322\n",
      "323\n",
      "324\n",
      "325\n",
      "326\n",
      "327\n",
      "328\n",
      "329\n",
      "330\n",
      "331\n",
      "332\n",
      "333\n",
      "334\n",
      "335\n",
      "336\n",
      "337\n",
      "338\n",
      "339\n",
      "340\n",
      "341\n",
      "342\n",
      "343\n",
      "344\n",
      "345\n",
      "346\n",
      "347\n",
      "348\n",
      "349\n",
      "350\n",
      "351\n",
      "352\n",
      "353\n",
      "354\n",
      "355\n",
      "356\n",
      "357\n",
      "358\n",
      "359\n",
      "360\n",
      "361\n",
      "362\n",
      "363\n",
      "364\n",
      "365\n",
      "366\n",
      "367\n",
      "368\n",
      "369\n",
      "370\n",
      "371\n",
      "372\n",
      "373\n",
      "374\n",
      "375\n",
      "376\n",
      "377\n",
      "378\n",
      "379\n",
      "380\n",
      "381\n",
      "382\n",
      "383\n",
      "384\n",
      "385\n",
      "386\n",
      "387\n",
      "388\n",
      "389\n",
      "390\n",
      "391\n",
      "392\n",
      "393\n",
      "394\n",
      "395\n",
      "396\n",
      "397\n",
      "398\n",
      "399\n",
      "400\n",
      "401\n",
      "402\n",
      "403\n",
      "404\n",
      "405\n",
      "406\n",
      "407\n",
      "408\n",
      "409\n",
      "410\n",
      "411\n",
      "412\n",
      "413\n",
      "414\n",
      "415\n",
      "416\n",
      "417\n",
      "418\n",
      "419\n",
      "420\n",
      "421\n",
      "422\n",
      "423\n",
      "424\n",
      "425\n",
      "426\n",
      "427\n",
      "428\n",
      "429\n",
      "430\n",
      "431\n",
      "432\n",
      "433\n",
      "434\n",
      "435\n",
      "436\n",
      "437\n",
      "438\n",
      "439\n",
      "440\n",
      "441\n",
      "442\n",
      "443\n",
      "444\n",
      "445\n",
      "446\n",
      "447\n",
      "448\n",
      "449\n",
      "450\n",
      "451\n",
      "452\n",
      "453\n",
      "454\n",
      "455\n",
      "456\n",
      "457\n",
      "458\n",
      "459\n",
      "460\n",
      "461\n",
      "462\n",
      "463\n",
      "464\n",
      "465\n",
      "466\n",
      "467\n",
      "468\n",
      "469\n",
      "470\n",
      "471\n",
      "472\n",
      "473\n",
      "474\n",
      "475\n",
      "476\n",
      "477\n",
      "478\n",
      "479\n",
      "480\n",
      "481\n",
      "482\n",
      "483\n",
      "484\n",
      "485\n",
      "486\n",
      "487\n",
      "488\n",
      "489\n",
      "490\n",
      "491\n",
      "492\n",
      "493\n",
      "494\n",
      "495\n",
      "496\n",
      "497\n",
      "498\n",
      "499\n",
      "500\n",
      "501\n",
      "502\n",
      "503\n",
      "504\n",
      "505\n",
      "506\n",
      "507\n",
      "508\n",
      "509\n",
      "510\n",
      "511\n",
      "512\n",
      "513\n",
      "514\n",
      "515\n",
      "516\n",
      "517\n",
      "518\n",
      "519\n",
      "520\n",
      "521\n",
      "522\n",
      "523\n",
      "524\n",
      "525\n",
      "526\n",
      "527\n",
      "528\n",
      "529\n",
      "530\n",
      "531\n",
      "532\n",
      "533\n",
      "534\n",
      "535\n",
      "536\n",
      "537\n",
      "538\n",
      "539\n",
      "540\n",
      "541\n",
      "542\n",
      "543\n",
      "544\n",
      "545\n",
      "546\n",
      "547\n",
      "548\n",
      "549\n",
      "550\n",
      "551\n",
      "552\n",
      "553\n",
      "554\n",
      "555\n",
      "556\n",
      "557\n",
      "558\n",
      "559\n",
      "560\n",
      "561\n",
      "562\n",
      "563\n",
      "564\n",
      "565\n",
      "566\n",
      "567\n",
      "568\n",
      "569\n",
      "570\n",
      "571\n",
      "572\n",
      "573\n",
      "574\n",
      "575\n",
      "576\n",
      "577\n",
      "578\n",
      "579\n",
      "580\n",
      "581\n",
      "582\n",
      "583\n",
      "584\n",
      "585\n",
      "586\n",
      "587\n",
      "588\n",
      "589\n",
      "590\n",
      "591\n",
      "592\n",
      "593\n",
      "594\n",
      "595\n",
      "596\n",
      "597\n",
      "598\n",
      "599\n",
      "600\n",
      "601\n",
      "602\n",
      "603\n",
      "604\n",
      "605\n",
      "606\n",
      "607\n",
      "608\n",
      "609\n",
      "610\n",
      "611\n",
      "612\n",
      "613\n",
      "614\n",
      "615\n",
      "616\n",
      "617\n",
      "618\n",
      "619\n",
      "620\n",
      "621\n",
      "622\n",
      "623\n",
      "624\n",
      "625\n",
      "626\n",
      "627\n",
      "628\n",
      "629\n",
      "630\n",
      "631\n",
      "632\n",
      "633\n",
      "634\n",
      "635\n",
      "636\n",
      "637\n",
      "638\n",
      "639\n",
      "640\n",
      "641\n",
      "642\n",
      "643\n",
      "644\n",
      "645\n",
      "646\n",
      "647\n",
      "648\n",
      "649\n",
      "650\n",
      "651\n",
      "652\n",
      "653\n",
      "654\n",
      "655\n",
      "656\n",
      "657\n",
      "658\n",
      "659\n",
      "660\n",
      "661\n",
      "662\n",
      "663\n",
      "664\n",
      "665\n",
      "666\n",
      "667\n",
      "668\n",
      "669\n",
      "670\n",
      "671\n",
      "672\n",
      "673\n",
      "674\n",
      "675\n",
      "676\n",
      "677\n",
      "678\n",
      "679\n",
      "680\n",
      "681\n",
      "682\n",
      "683\n",
      "684\n",
      "685\n",
      "686\n",
      "687\n",
      "688\n",
      "689\n",
      "690\n",
      "691\n",
      "692\n",
      "693\n",
      "694\n",
      "695\n",
      "696\n",
      "697\n",
      "698\n",
      "699\n",
      "700\n",
      "701\n",
      "702\n",
      "703\n",
      "704\n",
      "705\n",
      "706\n",
      "707\n",
      "708\n",
      "709\n",
      "710\n",
      "711\n",
      "712\n",
      "713\n",
      "714\n",
      "715\n",
      "716\n",
      "717\n"
     ]
    },
    {
     "ename": "BadRequestError",
     "evalue": "Error code: 400 - {'type': 'error', 'error': {'type': 'invalid_request_error', 'message': 'Your credit balance is too low to access the Anthropic API. Please go to Plans & Billing to upgrade or purchase credits.'}}",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mBadRequestError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[79], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m random\u001b[38;5;241m.\u001b[39mseed(\u001b[38;5;241m42\u001b[39m)\n\u001b[1;32m      2\u001b[0m indices \u001b[38;5;241m=\u001b[39m random\u001b[38;5;241m.\u001b[39msample(\u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(statements)), \u001b[38;5;28mmin\u001b[39m(sampleSize, \u001b[38;5;28mlen\u001b[39m(statements)))\n\u001b[0;32m----> 3\u001b[0m sar_percentage_prompted, no_sar_percentage_prompted, total_sar, total_no_sar, noSarCT, isSarCT \u001b[38;5;241m=\u001b[39m \u001b[43mbeginPrompting\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstatements\u001b[49m\u001b[43m,\u001b[49m\u001b[43manswers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindices\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# print(\"______________________________________________________________________\")\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# print(f\"Prompted sarcastic statement recognition: {sar_percentage_prompted}%\")\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# print(f\"Total correct: {isSarCT}\")\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m# print(f\"No prompt non sarcastic statement recognition: {no_sar_percentage_base}%\")\u001b[39;00m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m# print(f\"Total correct: {zeroNoSarCT}\")\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[77], line 22\u001b[0m, in \u001b[0;36mbeginPrompting\u001b[0;34m(inputs, answers, with_prompt, indices)\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_samples):\n\u001b[1;32m     21\u001b[0m     time\u001b[38;5;241m.\u001b[39msleep(\u001b[38;5;241m5\u001b[39m)\n\u001b[0;32m---> 22\u001b[0m     isSarcastic \u001b[38;5;241m=\u001b[39m \u001b[43mgetResultWPrompt\u001b[49m\u001b[43m(\u001b[49m\u001b[43msampled_inputs\u001b[49m\u001b[43m[\u001b[49m\u001b[43mx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mif\u001b[39;00m with_prompt \u001b[38;5;28;01melse\u001b[39;00m getResultNoPrompt(sampled_inputs[x])\n\u001b[1;32m     23\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m sampled_answers[x] \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m     24\u001b[0m         totSarCt \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "Cell \u001b[0;32mIn[76], line 40\u001b[0m, in \u001b[0;36mgetResultWPrompt\u001b[0;34m(sample_input)\u001b[0m\n\u001b[1;32m     28\u001b[0m judgeInput \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m sarc_response\u001b[38;5;241m.\u001b[39mcontent[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mtext\u001b[38;5;241m.\u001b[39mstrip()\n\u001b[1;32m     30\u001b[0m reflect_system_prompt \u001b[38;5;241m=\u001b[39m (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou will be given a message from a political debate on the internet and a preliminary analysis on the statement. Summarize the preliminary analysis\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     31\u001b[0m                        \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDecide whether statement is sarcastic or not by first analyzing the following:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     32\u001b[0m                        \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mThe Implicature - What is implied in the conversation beyond the literal meaning?\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     37\u001b[0m                        \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mMeaning- What is the difference between the literal and implied meaning of the statement?\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     38\u001b[0m                        \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mReflect on the preliminary analysis and what should change, then decide if the statment is sarcastic.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 40\u001b[0m reflection_response \u001b[38;5;241m=\u001b[39m \u001b[43manthropic\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmessages\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     41\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mclaude-3-5-sonnet-20240620\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     42\u001b[0m \u001b[43m    \u001b[49m\u001b[43msystem\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreflect_system_prompt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     43\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmessages\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\n\u001b[1;32m     44\u001b[0m \u001b[43m        \u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrole\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43muser\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcontent\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43msample_input\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m \u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mjudgeInput\u001b[49m\u001b[43m}\u001b[49m\n\u001b[1;32m     45\u001b[0m \u001b[43m    \u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     46\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m4000\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     47\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1.0\u001b[39;49m\n\u001b[1;32m     48\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     50\u001b[0m judgeInput \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m- - - - - - - - - - - - - - - - - - - - - - - - - - - \u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     51\u001b[0m judgeInput \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m reflection_response\u001b[38;5;241m.\u001b[39mcontent[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mtext\u001b[38;5;241m.\u001b[39mstrip()\n",
      "File \u001b[0;32m/opt/homebrew/Cellar/jupyterlab/4.2.3/libexec/lib/python3.12/site-packages/anthropic/_utils/_utils.py:274\u001b[0m, in \u001b[0;36mrequired_args.<locals>.inner.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    272\u001b[0m             msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMissing required argument: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mquote(missing[\u001b[38;5;241m0\u001b[39m])\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    273\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(msg)\n\u001b[0;32m--> 274\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/Cellar/jupyterlab/4.2.3/libexec/lib/python3.12/site-packages/anthropic/resources/messages.py:888\u001b[0m, in \u001b[0;36mMessages.create\u001b[0;34m(self, max_tokens, messages, model, metadata, stop_sequences, stream, system, temperature, tool_choice, tools, top_k, top_p, extra_headers, extra_query, extra_body, timeout)\u001b[0m\n\u001b[1;32m    881\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m model \u001b[38;5;129;01min\u001b[39;00m DEPRECATED_MODELS:\n\u001b[1;32m    882\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m    883\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe model \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m is deprecated and will reach end-of-life on \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mDEPRECATED_MODELS[model]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mPlease migrate to a newer model. Visit https://docs.anthropic.com/en/docs/resources/model-deprecations for more information.\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    884\u001b[0m         \u001b[38;5;167;01mDeprecationWarning\u001b[39;00m,\n\u001b[1;32m    885\u001b[0m         stacklevel\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m,\n\u001b[1;32m    886\u001b[0m     )\n\u001b[0;32m--> 888\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_post\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    889\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m/v1/messages\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    890\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmaybe_transform\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    891\u001b[0m \u001b[43m        \u001b[49m\u001b[43m{\u001b[49m\n\u001b[1;32m    892\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmax_tokens\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    893\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmessages\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    894\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmodel\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    895\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmetadata\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    896\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstop_sequences\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop_sequences\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    897\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstream\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    898\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43msystem\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43msystem\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    899\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtemperature\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    900\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtool_choice\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtool_choice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    901\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtools\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtools\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    902\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtop_k\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_k\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    903\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtop_p\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_p\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    904\u001b[0m \u001b[43m        \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    905\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmessage_create_params\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mMessageCreateParams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    906\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    907\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmake_request_options\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    908\u001b[0m \u001b[43m        \u001b[49m\u001b[43mextra_headers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_headers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra_query\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_query\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra_body\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_body\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\n\u001b[1;32m    909\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    910\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mMessage\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    911\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    912\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mStream\u001b[49m\u001b[43m[\u001b[49m\u001b[43mRawMessageStreamEvent\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    913\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/Cellar/jupyterlab/4.2.3/libexec/lib/python3.12/site-packages/anthropic/_base_client.py:1277\u001b[0m, in \u001b[0;36mSyncAPIClient.post\u001b[0;34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1263\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpost\u001b[39m(\n\u001b[1;32m   1264\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   1265\u001b[0m     path: \u001b[38;5;28mstr\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1272\u001b[0m     stream_cls: \u001b[38;5;28mtype\u001b[39m[_StreamT] \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1273\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ResponseT \u001b[38;5;241m|\u001b[39m _StreamT:\n\u001b[1;32m   1274\u001b[0m     opts \u001b[38;5;241m=\u001b[39m FinalRequestOptions\u001b[38;5;241m.\u001b[39mconstruct(\n\u001b[1;32m   1275\u001b[0m         method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpost\u001b[39m\u001b[38;5;124m\"\u001b[39m, url\u001b[38;5;241m=\u001b[39mpath, json_data\u001b[38;5;241m=\u001b[39mbody, files\u001b[38;5;241m=\u001b[39mto_httpx_files(files), \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39moptions\n\u001b[1;32m   1276\u001b[0m     )\n\u001b[0;32m-> 1277\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(ResponseT, \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream_cls\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[0;32m/opt/homebrew/Cellar/jupyterlab/4.2.3/libexec/lib/python3.12/site-packages/anthropic/_base_client.py:954\u001b[0m, in \u001b[0;36mSyncAPIClient.request\u001b[0;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[1;32m    951\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    952\u001b[0m     retries_taken \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m--> 954\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    955\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    956\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    957\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    958\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream_cls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    959\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretries_taken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mretries_taken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    960\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/Cellar/jupyterlab/4.2.3/libexec/lib/python3.12/site-packages/anthropic/_base_client.py:1058\u001b[0m, in \u001b[0;36mSyncAPIClient._request\u001b[0;34m(self, cast_to, options, retries_taken, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1055\u001b[0m         err\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mread()\n\u001b[1;32m   1057\u001b[0m     log\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRe-raising status error\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m-> 1058\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_status_error_from_response(err\u001b[38;5;241m.\u001b[39mresponse) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1060\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_process_response(\n\u001b[1;32m   1061\u001b[0m     cast_to\u001b[38;5;241m=\u001b[39mcast_to,\n\u001b[1;32m   1062\u001b[0m     options\u001b[38;5;241m=\u001b[39moptions,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1066\u001b[0m     retries_taken\u001b[38;5;241m=\u001b[39mretries_taken,\n\u001b[1;32m   1067\u001b[0m )\n",
      "\u001b[0;31mBadRequestError\u001b[0m: Error code: 400 - {'type': 'error', 'error': {'type': 'invalid_request_error', 'message': 'Your credit balance is too low to access the Anthropic API. Please go to Plans & Billing to upgrade or purchase credits.'}}"
     ]
    }
   ],
   "source": [
    "sar_percentage_prompted, no_sar_percentage_prompted, total_sar, total_no_sar, noSarCT, isSarCT = beginPrompting(statements,answers, True, indices)\n",
    "# print(\"______________________________________________________________________\")\n",
    "# print(f\"Prompted sarcastic statement recognition: {sar_percentage_prompted}%\")\n",
    "# print(f\"Total correct: {isSarCT}\")\n",
    "# print(f\"Prompted non sarcastic statement recognition: {no_sar_percentage_prompted}%\")\n",
    "# print(f\"Total correct: {noSarCT}\")\n",
    "# sar_percentage_base, no_sar_percentage_base, total_sar_no_prompt, total_no_sar_no_prompt, zeroNoSarCT, zeroIsSarCT= beginPrompting(utterances, ans,contexts, speakers, current_speaker, False, indices)\n",
    "# print(f\"No prompt sarcastic statement recognition: {sar_percentage_base}%\")\n",
    "# print(f\"Total correct: {zeroIsSarCT}\")\n",
    "# print(f\"No prompt non sarcastic statement recognition: {no_sar_percentage_base}%\")\n",
    "# print(f\"Total correct: {zeroNoSarCT}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "d0e7c19e-30a0-476e-bc4b-92fee1fbe177",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "718\n"
     ]
    }
   ],
   "source": [
    "print(len(outputs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "cca2deb2-06d2-4581-85a0-ac5cdda08351",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wow i love only ever buying gray and striped shirts\n"
     ]
    }
   ],
   "source": [
    "print(statements[161])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "9437ad67-7a8e-479b-a94a-ed8e908c7392",
   "metadata": {
    "id": "9437ad67-7a8e-479b-a94a-ed8e908c7392"
   },
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "now = datetime.now()\n",
    "\n",
    "# Format the date and time as a string\n",
    "formatted_datetime = now.strftime(\"%m-%d-%Y %H:%M:%S\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "f7651842-1f74-40b5-8e4c-2a5a5a6e9179",
   "metadata": {
    "id": "f7651842-1f74-40b5-8e4c-2a5a5a6e9179"
   },
   "outputs": [],
   "source": [
    "with open(\"SemEval2018_V1.txt\", \"a\") as file:\n",
    "    file.write(f\"\\nSarcasm Corpus V1 ({formatted_datetime}, Total Sample Size : {sampleSize}): \\n\")\n",
    "    file.write(f\"Prompted sarcastic statement recognition ({total_sar} total) : {sar_percentage_prompted}% \\n\")\n",
    "    file.write(f\"Total correct: {isSarCT}\\n\")\n",
    "    file.write(f\"Prompted non sarcastic statement recognition ({total_no_sar} total) : {no_sar_percentage_prompted}%\\n\")\n",
    "    file.write(f\"Total correct: {noSarCT}\\n\")\n",
    "    # file.write(f\"No prompt sarcastic statement recognition ({total_sar_no_prompt} total) : {sar_percentage_base}%\\n\")\n",
    "    # file.write(f\"Total correct: {zeroIsSarCT}\")\n",
    "    # file.write(f\"No prompt non sarcastic statement recognition ({total_no_sar_no_prompt} total) : {no_sar_percentage_base}%\\n\")\n",
    "    # file.write(f\"Total correct: {zeroNoSarCT}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "3604e540-18e5-424e-91e7-b381db56ba64",
   "metadata": {
    "id": "3604e540-18e5-424e-91e7-b381db56ba64"
   },
   "outputs": [],
   "source": [
    "with open(\"SemEval2018_results_V1.txt\" ,\"a\") as file:\n",
    "    for x in range(len(outputs)) :\n",
    "        file.write (f\"\\n{outputs[x]} \\n- - - - - - - - - - - - - - - - - - - - - - - - - - - \\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "7fc836e0-5d67-4f7f-9c53-932e12214cfd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3468\n"
     ]
    }
   ],
   "source": [
    "print(len(statements))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "acc50f2b-f469-421e-a959-95ef4825093c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ANSWER: \n",
      "- - - - - - - - - - - - - - - - - - - - - - - - - - - \n",
      "Statement to analyze: \"ok wow…unfollowing now. thought he was the cool new kid from hollywood but apparently he's a virgin who lit the black flame candle\"\n",
      "\n",
      "Analysis:\n",
      "\n",
      "What the speaker implies about the situation:\n",
      "The speaker implies that they previously had a positive opinion of someone (likely a celebrity or public figure) but have now drastically changed their view based on some recent information or action. They suggest that this person has done something foolish or problematic, using a reference to the movie \"Hocus Pocus\" to illustrate their point.\n",
      "\n",
      "What the speaker thinks about the situation:\n",
      "The speaker is disappointed and disillusioned with the person they're referring to. They had initially viewed this individual as cool and promising, but now see them as naive, inexperienced, or having made a significant mistake. The speaker is expressing their decision to withdraw support or interest in this person.\n",
      "\n",
      "Are what the speaker implies and what the speaker thinks saying the same thing?\n",
      "Yes, in this case, what the speaker implies and what they think are closely aligned. Both convey disappointment, a change in opinion, and a decision to distance themselves from the subject of their comment.\n",
      "\n",
      "Is the speaker pretending to have a certain attitude toward the conversation?\n",
      "No, the speaker does not appear to be pretending to have a certain attitude. Their statement seems to be a genuine expression of disappointment and frustration. The casual language and pop culture reference suggest sincerity rather than a pretense. The speaker is openly sharing their change of heart and decision to unfollow, which indicates a straightforward attitude toward the conversation.\n",
      "- - - - - - - - - - - - - - - - - - - - - - - - - - - \n",
      "Summary of preliminary analysis:\n",
      "The analysis suggests that the speaker is expressing genuine disappointment and a change of opinion about someone they previously admired. They're using casual language and a pop culture reference to convey their disillusionment, implying that the person in question has done something foolish or problematic. The speaker's implied meaning aligns with their thoughts, indicating sincerity rather than pretense.\n",
      "\n",
      "Sarcasm analysis:\n",
      "\n",
      "1. Implicature: The speaker implies that the person they're referring to has made a significant mistake or revealed themselves to be naive/inexperienced, contrary to their initial impression.\n",
      "\n",
      "2. Presuppositions: The speaker assumes the audience knows who \"he\" is and understands the \"Hocus Pocus\" reference.\n",
      "\n",
      "3. Intent: The speaker aims to express their disappointment and decision to withdraw support, possibly influencing others' opinions as well.\n",
      "\n",
      "4. Polarity: The last sentence has a negative tone, expressing disappointment and criticism.\n",
      "\n",
      "5. Pretense: There doesn't appear to be pretense in the speaker's attitude. The statement seems to be a genuine expression of frustration.\n",
      "\n",
      "6. Meaning: The literal meaning (unfollowing someone who lit a black flame candle) differs from the implied meaning (withdrawing support from someone who made a significant mistake or revealed an undesirable trait).\n",
      "\n",
      "Reflection:\n",
      "While the preliminary analysis is generally accurate, it might underestimate the potential for sarcasm. The extreme shift from \"cool new kid\" to \"virgin who lit the black flame candle\" could be seen as an exaggerated reaction, potentially indicating sarcasm.\n",
      "\n",
      "Decision:\n",
      "Despite the potential for sarcasm, I believe this statement is not sarcastic. The abrupt tone shift and pop culture reference are more likely used for emphasis and humor rather than to convey the opposite of what's stated. The speaker seems genuinely disappointed and is using colorful language to express their feelings, but their core message appears sincere.\n",
      "- - - - - - - - - - - - - - - - - - - - - - - - - - - \n",
      "\n",
      "Judgement: NO\n",
      "\n",
      "ANSWER: 0\n",
      "Prompt sarcastic statement recognition (115 total)\n",
      "Total Prompted sarcastic statement  (169 total)  \n",
      "Non sarcastic statement recognition (485 total)\n",
      "Total Prompted non sarcastic statement (549 total)\n",
      "\n",
      "['At this point I just feel bad for him 😂 https://t.co/nEt3JPAsAM', 'im not ignoring you... I’m just ignoring my responsibilities', \"Woke up at 3 am with a MAD craving for pickles. I guess that means I'm expecting!\", 'Friday Night Dinners will never be the same. So sad. #PaulRitter', 'The Drake and Josh theme song, “I Found a Way”, is the best Nickelodeon TV theme song and that is a fact', \"I'm gonna have a breakdown.\", 'The tiktok fyp knows me better than I know myself', \"I hate how anti vaxxers have ruined everything. Trying to find information on vaccine side effects and it's plagued with morons\", 'locals saying “not all cops” are missing the point and showing their underlying racism that we all knew was there in the first place', \"I'm big glad I'm not a Sonic fan because Sega gassed them mfers up for a lot of nothing.\"]\n"
     ]
    }
   ],
   "source": [
    "# import random\n",
    "\n",
    "# import pandas as pd\n",
    "# import glob\n",
    "# import os\n",
    "\n",
    "# path = \"../data/iSarcasmEval.csv\"\n",
    "# df = pd.read_csv(path);\n",
    "# random.seed(42)\n",
    "# answers = df['sarcastic']\n",
    "# statements = df['tweet']\n",
    "\n",
    "# print(outputs[717])\n",
    "\n",
    "# indices = random.sample(range(714,len(statements)), 3468-714)\n",
    "# sampled_inputs = [statements[i] for i in indices]\n",
    "# sampled_answers = [answers[i] for i in indices]\n",
    "# print(sampled_inputs[:10])"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
