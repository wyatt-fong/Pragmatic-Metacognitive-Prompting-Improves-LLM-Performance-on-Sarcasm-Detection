{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e1045e13-3cba-441f-bf34-18b11d3646e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "import re\n",
    "import random\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "62d219a1-6872-4ff1-be2c-eabeba850b08",
   "metadata": {},
   "outputs": [],
   "source": [
    "openai.api_key = 'sk-proj-WitfuXAxR0xmNWKEcpNHT3BlbkFJY1C3hJwS1p6kXcbFnEtW'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "136e5a33-6f84-4039-9738-c4b7d9e33f4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "json_file_path = 'data/MUStARD_data.json'\n",
    "\n",
    "with open(json_file_path, 'r') as file:\n",
    "    data = json.load(file)\n",
    "df = pd.DataFrame.from_dict(data, orient='index')\n",
    "\n",
    "utterances = df['utterance']\n",
    "ans = df['sarcasm']\n",
    "contexts = df['context']\n",
    "speakers = df['context_speakers']\n",
    "current_speaker = df['speaker']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "99fe05e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs =[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b9d1e110-87dc-493b-8aa3-0bccbc1a6058",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getResultNoPrompt (speaker_dialogue, dialogue) :\n",
    "    prompt = f\"\\\"{' '.join(dialogue) + speaker_dialogue}\\\"\\n You will be given movie or tv show dialogue. Determine whether the last statement is sarcastic with YES or NO.\"\n",
    "    response = openai.chat.completions.create(\n",
    "        model='gpt-4o-mini-2024-07-18',\n",
    "        messages= [\n",
    "            {\"role\": \"user\", \"content\" : prompt}\n",
    "        ],\n",
    "        max_tokens = 10,\n",
    "        temperature = 1.0\n",
    "    )\n",
    "    output = response.choices[0].message.content.strip();\n",
    "    isSarcastic = True if \"yes\" in output.lower() else False\n",
    "    return isSarcastic;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "41560bb2-bba6-42ce-9312-92c02dc167e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getResultWPrompt(speaker_dialogue, dialogue):\n",
    "    judgeInput = \"\"  \n",
    "    output = \"\" \n",
    "    \n",
    "    #System prompt for sarcasm justification\n",
    "    sarc_system_prompt = (\"You will be given movie or tv show dialogue, and will analyze the statement marked between brackets. Summarize the conversation, and repeat back the statement to analyze.\"\n",
    "                           \"Then, analyze the following:\\n\"\n",
    "                          \"-What does the speaker imply about the situation with their statement?\\n\"\n",
    "                          \"-What does the speaker think about the situation?\\n\"\n",
    "                          \"-Are what the speaker implies and what the speaker thinks saying the same thing?\\n\"\n",
    "                          \"Finally, decide if the speaker is pretending to have a certain attitude toward the conversation.\"\n",
    "                          )\n",
    "\n",
    "    sarc_response = openai.chat.completions.create(\n",
    "        model='gpt-4o',\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": sarc_system_prompt},\n",
    "            {\"role\": \"user\", \"content\": ' '.join(dialogue) + \"{\"+ speaker_dialogue + \"}\"}\n",
    "        ],\n",
    "        max_tokens= 2000,\n",
    "        temperature=1.0\n",
    "    )\n",
    "    print(' '.join(dialogue) + \"{\"+ speaker_dialogue + \"}\")\n",
    "    judgeInput += \"\\n- - - - - - - - - - - - - - - - - - - - - - - - - - - \\n\"\n",
    "    judgeInput += sarc_response.choices[0].message.content.strip()\n",
    "\n",
    "    reflect_system_prompt = (\"You will be given a piece of movie dialogue, a statment marked in brackets, and a preliminary analysis on the marked statement. Summarize the preliminary analysis\"\n",
    "                             \"Decide whether statement is sarcastic or not by first analyzing the following:\\n\"\n",
    "                            \"\\nThe Implicature - What is implied in the conversation beyond the literal meaning?\"\n",
    "                            \"\\nThe Presuppositions - What information in the conversation is taken for granted?\"\n",
    "                            \"\\nThe intent of the speaker - What do the speaker(s) hope to achieve with their statement and who are the speakers?\\n\"\n",
    "                            \"\\nThe polarity - Does the last sentence have a positive or negative tone?\"\n",
    "                            \"\\nPretense - Is there pretense in the speaker's attitude?\"\n",
    "                            \"\\nMeaning- What is the difference between the literal and implied meaning of the statement?\"\n",
    "                             \"Reflect on the preliminary analysis and what should change, then decide if the statment is sarcastic.\")\n",
    "                            \n",
    "                       \n",
    "    reflection_response = openai.chat.completions.create(\n",
    "        model='gpt-4o',\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": reflect_system_prompt},\n",
    "            {\"role\": \"user\", \"content\": ' '.join(dialogue) + \"{\"+ speaker_dialogue + \"}\" + judgeInput}\n",
    "        ],\n",
    "        max_tokens=4000,\n",
    "        temperature=1.0\n",
    "    )\n",
    "    \n",
    "    judgeInput += \"\\n- - - - - - - - - - - - - - - - - - - - - - - - - - - \\n\"\n",
    "    judgeInput += reflection_response.choices[0].message.content.strip()\n",
    "    judgeInput += \"\\n- - - - - - - - - - - - - - - - - - - - - - - - - - - \\n\"\n",
    "    output += (f\"\\nANSWER: {judgeInput}\\n\")\n",
    "\n",
    "    clean_system_prompt = (\n",
    "        \"You will be given the output of an LLM which decided if a sentence is sarcastic or not. \"\n",
    "        \"Read the output, then summarize the LLM's stance with ONLY a YES (they think the sentence is sarcastic) or NO (they think the sentence is not sarcastic).\"\n",
    "    )\n",
    "    clean_output = openai.chat.completions.create(\n",
    "        model='gpt-4o',\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": clean_system_prompt},\n",
    "            {\"role\": \"user\", \"content\": judgeInput}\n",
    "        ],\n",
    "        max_tokens=10,\n",
    "        temperature=1.0\n",
    "    )\n",
    "    clean_output = clean_output.choices[0].message.content.strip()\n",
    "    output += (f\"Judgement: {clean_output}\\n\")\n",
    "\n",
    "    # Store output and determine if the statement is sarcastic\n",
    "    outputs.append(output)\n",
    "    # Determine sarcasm based on clean output\n",
    "    isSarcastic = True if \"yes\" in clean_output.lower() else False\n",
    "    return isSarcastic\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44773b51",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "3aed0c29-e06f-4676-8851-80e4a540825b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "def beginPrompting(inputs, answers,context, speakers, current_speaker, with_prompt, indices):\n",
    "    num_samples = len(indices)\n",
    "    with open(\"MUSTARD_reflection-mistakes.txt\" ,\"a\") as file:\n",
    "            file.write (f\"\\nIncorrect Answers: (Full output listed below) \\n- - - - - - - - - - - - - - - - - - - - - - - - - - - \\n\")\n",
    "\n",
    "    sampled_inputs = [inputs.iloc[i] for i in indices]\n",
    "    sampled_answers = [answers.iloc[i] for i in indices]\n",
    "    sampled_contexts = [context.iloc[i] for i in indices]\n",
    "    sampled_speakers = [speakers.iloc[i] for i in indices]\n",
    "    sampled_current_speaker = [current_speaker.iloc[i] for i in indices]\n",
    "    \n",
    "    isSarCt = 0\n",
    "    noSarCt = 0\n",
    "    totNoSarCt = 0\n",
    "    totSarCt = 0\n",
    "\n",
    "    for x in range(num_samples):\n",
    "        dialogue = [f\"{speaker}: {utterance}\" for speaker, utterance in zip(sampled_speakers[x], sampled_contexts[x])]\n",
    "        speaker_dialogue = sampled_current_speaker[x] + \": \" + sampled_inputs[x]\n",
    "        isSarcastic = getResultWPrompt(speaker_dialogue, dialogue) if with_prompt else getResultNoPrompt(speaker_dialogue, dialogue)\n",
    "        if sampled_answers[x]:\n",
    "            totSarCt += 1\n",
    "            if(isSarcastic):\n",
    "                isSarCt += 1\n",
    "            else:\n",
    "                with open(\"MUSTARD_reflection-mistakes.txt\" ,\"a\") as file:\n",
    "                    file.write (f\"\\n{outputs[x]} \\n- - - - - - - - - - - - - - - - - - - - - - - - - - - \\n\")\n",
    "                    file.write (f\"\\nANSWER: {sampled_answers[x]}\\n\")\n",
    "        else:\n",
    "            totNoSarCt += 1\n",
    "            if(not isSarcastic):\n",
    "                noSarCt += 1\n",
    "            else:\n",
    "                with open(\"MUSTARD_reflection-mistakes.txt\" ,\"a\") as file:\n",
    "                    file.write (f\"\\n{outputs[x]} \\n- - - - - - - - - - - - - - - - - - - - - - - - - - - \\n\")\n",
    "                    file.write (f\"\\nANSWER: {sampled_answers[x]}\\n\")\n",
    "\n",
    "        if with_prompt:\n",
    "            outputs[x]+=(f\"\\nANSWER: {sampled_answers[x]}\\n\")\n",
    "            outputs[x]+=(f\"Prompt sarcastic statement recognition ({isSarCt} total)\\n\")\n",
    "            outputs[x]+=(f\"Total Prompted sarcastic statement  ({totSarCt} total)  \\n\")\n",
    "            outputs[x]+=(f\"Non sarcastic statement recognition ({noSarCt} total)\\n\")\n",
    "            outputs[x]+=(f\"Total Prompted non sarcastic statement ({totNoSarCt} total)\\n\")\n",
    "            print(\"----------\")\n",
    "            print(f\"Prompt sarcastic statement recognition ({isSarCt} total)\")\n",
    "            print(f\"Total Prompted sarcastic statement  ({totSarCt} total)  \")\n",
    "            print(f\"Non sarcastic statement recognition ({noSarCt} total)\")\n",
    "            print(f\"Total Prompted non sarcastic statement ({totNoSarCt} total)\")\n",
    "            print(\"----------\")\n",
    "             \n",
    "    sar_percentage = (isSarCt / totSarCt * 100) if totSarCt > 0 else 0\n",
    "    no_sar_percentage = (noSarCt / totNoSarCt * 100) if totNoSarCt > 0 else 0\n",
    "\n",
    "    return sar_percentage, no_sar_percentage, totSarCt, totNoSarCt, noSarCt, isSarCt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "8be4235d-3523-4144-be6b-488c013dbfe3",
   "metadata": {},
   "outputs": [],
   "source": [
    "sampleSize = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75b5f647-b82c-4aa0-a56f-7b651a7847c8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "random.seed(42)\n",
    "indices = random.sample(range(len(utterances)), min(sampleSize, len(utterances)))\n",
    "sar_percentage_prompted, no_sar_percentage_prompted, total_sar, total_no_sar, noSarCT, isSarCT = beginPrompting(utterances, ans, contexts, speakers, current_speaker, True, indices)\n",
    "print(\"______________________________________________________________________\")\n",
    "print(f\"Prompted sarcastic statement recognition: {sar_percentage_prompted}%\")\n",
    "print(f\"Total correct: {isSarCT}\")\n",
    "print(f\"Prompted non sarcastic statement recognition: {no_sar_percentage_prompted}%\")\n",
    "print(f\"Total correct: {noSarCT}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdf9d09c",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "9437ad67-7a8e-479b-a94a-ed8e908c7392",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "now = datetime.now()\n",
    "\n",
    "# Format the date and time as a string\n",
    "formatted_datetime = now.strftime(\"%m-%d-%Y %H:%M:%S\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "f7651842-1f74-40b5-8e4c-2a5a5a6e9179",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"MUSTARD_reflection.txt\", \"a\") as file:\n",
    "    file.write(f\"\\nMUStARD Dataset ({formatted_datetime}, Total Sample Size : {sampleSize}): \\n\")\n",
    "    file.write(f\"Prompted sarcastic statement recognition ({total_sar} total) : {sar_percentage_prompted}% \\n\")\n",
    "    file.write(f\"Total correct: {isSarCT}\\n\")\n",
    "    file.write(f\"Prompted non sarcastic statement recognition ({total_no_sar} total) : {no_sar_percentage_prompted}%\\n\")\n",
    "    file.write(f\"Total correct: {noSarCT}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "3604e540-18e5-424e-91e7-b381db56ba64",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"MUSTARD_reflection.txt\" ,\"a\") as file:\n",
    "    for x in range(len(outputs)) :\n",
    "        file.write (f\"\\n{outputs[x]} \\n- - - - - - - - - - - - - - - - - - - - - - - - - - - \\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
